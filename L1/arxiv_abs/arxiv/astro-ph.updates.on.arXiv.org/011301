Understanding gravitational wave emission from core-collapse supernovae will be essential for
their detection with current and future gravitational wave detectors. This requires a sample of
waveforms from modern 3D supernova simulations reaching well into the explosion phase, where gravitational
wave emission is expected to peak. However, recent waveforms from 3D simulations with multi-group
neutrino transport do not reach far into the explosion phase, and some are still obtained from non-exploding
models. We therefore calculate waveforms up to 0.9\,s after bounce using the neutrino hydrodynamics
code \textsc{CoCoNuT-FMT}. We consider two models with low and normal explosion energy, namely
explosions of an ultra-stripped progenitor with an initial helium star mass of $3.5\,M_{\odot}$,
and of an $18\,M_{\odot}$ single star. Both models show gravitational wave emission from the excitation
of surface g-modes in the proto-neutron star with frequencies between $\mathord{\sim}800\,\mathrm{Hz}$
and 1000\,Hz at peak emission. The peak amplitudes are about $6\, \mathrm{cm}$ and $10\, \mathrm{cm}$,
respectively, which is somewhat higher than in most recent 3D models of the pre-explosion or early
explosion phase. Using a Bayesian analysis, we determine the maximum detection distances for our
models in simulated Advanced LIGO, Advanced Virgo, and Einstein Telescope design sensitivity
noise. The more energetic $18 M_\odot$ explosion will be detectable to about $17.5 \,\mathrm{kpc}$
by the LIGO/Virgo network and to about $180\, \mathrm{kpc}$ with the Einstein Telescope. 