Gamma Ray Bursts (GRBs) visible up to very high redshift have become attractive targets as potential
new distance indicators. It is still not clear whether the relations proposed so far originate from
an unknown GRB physics or result from selection effects. We investigate this issue in the case of
the $L_X-T^*_a$ correlation (hereafter LT) between the X-ray luminosity $L_X (T_a)$ at the end
of the plateau phase, $T_a$, and the rest frame time $T^{*}_a$. We devise a general method to build
mock data sets starting from a GRB world model and taking into account selection effects on both time
and luminosity. This method shows how not knowing the efficiency function could influence the evaluation
of the intrinsic slope of any correlation and the GRB density rate. We investigate biases (small
offsets in slope or normalization) that would occur in the LT relation as a result of truncations,
possibly present in the intrinsic distributions of $L_X$ and $T^*_a$. We compare these results
with the ones in Dainotti et al. (2013) showing that in both cases the intrinsic slope of the LT correlation
is $\approx -1.0$. This method is general, therefore relevant to investigate if any other GRB correlation
is generated by the biases themselves. Moreover, because the farthest GRBs and star-forming galaxies
probe the reionization epoch, we evaluate the redshift-dependent ratio $\Psi(z)=(1+z)^{\alpha}$
of the GRB rate to star formation rate (SFR). We found a modest evolution $-0.2\leq \alpha \leq 0.5$
consistent with Swift GRB afterglow plateau in the redshift range $0.99<z<9.4$. 