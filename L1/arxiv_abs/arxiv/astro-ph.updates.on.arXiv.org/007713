A cosmological model, in which the cosmic microwave background (CMB) is a thermal radiation of intergalactic
dust instead of a relic radiation of the Big Bang, is revived and revisited. The model suggests that
a virtually transparent local Universe becomes considerably opaque at redshifts $z > 2-3$. Such
opacity is hardly to be detected in the Type Ia supernova data, but confirmed using quasar data. The
opacity steeply increases with redshift because of a high proper density of intergalactic dust
in the previous epochs. The temperature of intergalactic dust increases as $(1+z)$ and exactly
compensates the change of wavelengths due to redshift, so that the dust radiation looks apparently
like the radiation of the blackbody with a single temperature. The predicted dust temperature is
$T^{D} = 2.776 \, \mathrm{K}$, which differs from the CMB temperature by 1.9\% only, and the predicted
ratio between the total CMB and EBL intensities is 13.4 which is close to 12.5 obtained from observations.
The CMB temperature fluctuations are caused by EBL fluctuations produced by galaxy clusters and
voids in the Universe. The polarization anomalies of the CMB correlated with temperature anisotropies
are caused by the polarized thermal emission of needle-shaped conducting dust grains aligned by
large-scale magnetic fields around clusters and voids. A strong decline of the luminosity density
for $z > 4$ is interpreted as the result of high opacity of the Universe rather than of a decline of the
global stellar mass density at high redshifts. 