We present a systematic study of the pure deflagration model of Type Ia supernovae using three-dimensional,
high-resolution, full-star hydrodynamical simulations, nucleosynthetic yields calculated
using Lagrangian tracer particles, and light curves calculated using radiation transport. We
evaluate the simulations by comparing their predicted light curves with many observed SNe Ia using
the SALT2 data-driven model and find that the simulations may correspond to under-luminous SNe
Iax. We explore the effects of the initial conditions on our results by varying the number of randomly
selected ignition points from 63 to 3500, and the radius of the centered sphere they are confined
in from 128 to 384 km. We find that the rate of nuclear burning depends on the number of ignition points
at early times, the density of ignition points at intermediate times, and the radius of the confining
sphere at late times. The results depend primarily on the number of ignition points, but we do not
expect this to be the case in general. The simulations with few ignition points release more nuclear
energy $E_{\mathrm{nuc}}$, have larger kinetic energies $E_{\mathrm{K}}$, and produce more
$^{56}$Ni than those with many ignition points, and differ in the distribution of $^{56}$Ni, Si,
and C/O in the ejecta. For these reasons, the simulations with few ignition points exhibit higher
peak B-band absolute magnitudes $M_\mathrm{B}$ and light curves that rise and decline more quickly;
their $M_\mathrm{B}$ and light curves resemble those of under-luminous SNe Iax, while those for
simulations with many ignition points are not. 