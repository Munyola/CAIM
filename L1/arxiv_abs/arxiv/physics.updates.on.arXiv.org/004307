We show that statistical criticality, i.e. the occurrence of power law frequency distributions,
arises in samples that are most informative on the underlying generative process. In order to reach
this conclusion, we first identify the frequency with which different outcomes occur in a sample,
as the variable carrying useful information on the generative process. This differs from the entropy
of the data, that we take as a measure of resolution. The entropy of the frequency, that we call relevance,
provides an upper bound to the number of informative bits. Samples that maximise relevance at a given
resolution - that we call most informative samples - exhibit statistical criticality. We show how
this naturally arises from the concentration property of the Asymptotic Equipartition Property.
Within a thermodynamic analogy, we find that most informative representations of high dimensional
data arise from a principle of minimal entropy, at odds with equilibrium statistical mechanics
where the entropy is maximised. This is why, contrary to statistical mechanics, statistical criticality
requires no parameter fine tuning in most informative samples. In addition, Zipf's law arises at
the optimal trade-off between resolution (i.e. compression) and relevance. As a byproduct, we
derive an estimate of the maximal number of parameters that can be estimated from a dataset, in the
absence of prior knowledge on the generative model. We finally show how our findings can be derived
from an unsupervised version of the Information Bottleneck method. 