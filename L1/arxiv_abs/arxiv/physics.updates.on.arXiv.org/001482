Despite the progress in high performance computing, Computational Fluid Dynamics (CFD) simulations
are still computationally expensive for many practical engineering applications such as simulating
large computational domains and highly turbulent flows. One of the major reasons of the high expense
of CFD is the need for a fine grid to resolve phenomena at the relevant scale, and obtain a grid-independent
solution. The fine grid requirements often drive the computational time step size down, which makes
long transient problems prohibitively expensive. In the research presented, the feasibility
of a Coarse Grid CFD (CG-CFD) approach is investigated by utilizing Machine Learning (ML) algorithms.
Relying on coarse grids increases the discretization error. Hence, a method is suggested to produce
a surrogate model that predicts the CG-CFD local errors to correct the variables of interest. Given
high-fidelity data, a surrogate model is trained to predict the CG-CFD local errors as a function
of the coarse grid local features. ML regression algorithms are utilized to construct a surrogate
model that relates the local error and the coarse grid features. This method is applied to a three-dimensional
flow in a lid driven cubic cavity domain. The performance of the method was assessed by training the
surrogate model on the flow full field spatial data and tested on new data (from flows of different
Reynolds number and/or computed by different grid sizes). The proposed method maximizes the benefit
of the available data and shows potential for a good predictive capability. 