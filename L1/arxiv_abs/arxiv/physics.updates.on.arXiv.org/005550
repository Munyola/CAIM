At HEP experiments, processing terabytes of structured numerical event data to a few statistical
summaries is a common task. This step involves selecting events and objects within the event, reconstructing
high-level variables, evaluating multivariate classifiers with up to hundreds of variations
and creating thousands of low-dimensional histograms. Currently, this is done using multi-step
workflows and batch jobs. Based on the CMS search for $H(\mu \mu)$, we demonstrate that it is possible
to carry out significant parts of a real collider analysis at a rate of up to a million events per second
on a single multicore server with optional GPU acceleration. This is achieved by representing HEP
event data as memory-mappable sparse arrays, and by expressing common analysis operations as kernels
that can be parallelized across the data using multithreading. We find that only a small number of
relatively simple kernels are needed to implement significant parts of this Higgs analysis. Therefore,
analysis of real collider datasets of billions events could be done within minutes to a few hours
using simple multithreaded codes, reducing the need for managing distributed workflows in the
exploratory phase. This approach could speed up the cycle for delivering physics results at HEP
experiments. We release the hepaccelerate prototype library as a demonstrator of such accelerated
computational kernels. We look forward to discussion, further development and use of efficient
and easy-to-use software for terabyte-scale high-level data analysis in the physical sciences.
