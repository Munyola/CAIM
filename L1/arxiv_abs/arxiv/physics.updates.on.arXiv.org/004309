Humans are the final decision makers in critical tasks that involve ethical and legal concerns,
ranging from recidivism prediction, to medical diagnosis, to fighting against fake news. Although
machine learning models can sometimes achieve impressive performance in these tasks, these tasks
are not amenable to full automation. To realize the potential of machine learning for improving
human decisions, it is important to understand how assistance from machine learning models affects
human performance and human agency. In this paper, we use deception detection as a testbed and investigate
how we can harness explanations and predictions of machine learning models to improve human performance
while retaining human agency. We propose a spectrum between full human agency and full automation,
and develop varying levels of machine assistance along the spectrum that gradually increase the
influence of machine predictions. We find that without showing predicted labels, explanations
alone slightly improve human performance in the end task. In comparison, human performance is greatly
improved by showing predicted labels (>20% relative improvement) and can be further improved by
explicitly suggesting strong machine performance. Interestingly, when predicted labels are
shown, explanations of machine predictions induce a similar level of accuracy as an explicit statement
of strong machine performance. Our results demonstrate a tradeoff between human performance and
human agency and show that explanations of machine predictions can moderate this tradeoff. 