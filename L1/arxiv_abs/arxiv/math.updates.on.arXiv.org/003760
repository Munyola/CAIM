In many situations across computational science and engineering, multiple computational models
are available that describe a system of interest. These different models have varying evaluation
costs and varying fidelities. Typically, a computationally expensive high-fidelity model describes
the system with the accuracy required by the current application at hand, while lower-fidelity
models are less accurate but computationally cheaper than the high-fidelity model. Outer-loop
applications, such as optimization, inference, and uncertainty quantification, require multiple
model evaluations at many different inputs, which often leads to computational demands that exceed
available resources if only the high-fidelity model is used. This work surveys multifidelity methods
that accelerate the solution of outer-loop applications by combining high-fidelity and low-fidelity
model evaluations, where the low-fidelity evaluations arise from an explicit low-fidelity model
(e.g., a simplified physics approximation, a reduced model, a data-fit surrogate, etc.) that approximates
the same output quantity as the high-fidelity model. The overall premise of these multifidelity
methods is that low-fidelity models are leveraged for speedup while the high-fidelity model is
kept in the loop to establish accuracy and/or convergence guarantees. We categorize multifidelity
methods according to three classes of strategies: adaptation, fusion, and filtering. The paper
reviews multifidelity methods in the outer-loop contexts of uncertainty propagation, inference,
and optimization. 