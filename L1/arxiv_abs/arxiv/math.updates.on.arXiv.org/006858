Last decade witnesses significant methodological and theoretical advances in estimating large
precision matrices. In particular, there are scientific applications such as longitudinal data,
meteorology and spectroscopy in which the ordering of the variables can be interpreted through
a bandable structure on the Cholesky factor of the precision matrix. However, the minimax theory
has still been largely unknown, as opposed to the well established minimax results over the corresponding
bandable covariance matrices. In this paper, we focus on two commonly used types of parameter spaces,
and develop the optimal rates of convergence under both the operator norm and the Frobenius norm.
A striking phenomenon is found: two types of parameter spaces are fundamentally different under
the operator norm but enjoy the same rate optimality under the Frobenius norm, which is in sharp contrast
to the equivalence of corresponding two types of bandable covariance matrices under both norms.
This fundamental difference is established by carefully constructing the corresponding minimax
lower bounds. Two new estimation procedures are developed: for the operator norm, our optimal procedure
is based on a novel local cropping estimator targeting on all principle submatrices of the precision
matrix while for the Frobenius norm, our optimal procedure relies on a delicate regression-based
thresholding rule. Lepski's method is considered to achieve optimal adaptation. We further establish
rate optimality in the nonparanormal model. Numerical studies are carried out to confirm our theoretical
findings. 