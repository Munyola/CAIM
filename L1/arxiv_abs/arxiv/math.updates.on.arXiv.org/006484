Recent work in machine learning has shown that optimization algorithms such as Nesterov's accelerated
gradient can be obtained as the discretization of a continuous dynamical system. Since different
discretizations can lead to different algorithms, it is important to choose the ones that preserve
certain structural properties of the dynamical system, such as critical points, stability and
convergence rates. In this paper we study structure-preserving discretizations for certain classes
of dissipative systems, which allow us to analyze properties of existing accelerated algorithms
as well as introduce new ones. In particular, we consider two classes of conformal Hamiltonian systems
whose trajectories lie on a symplectic manifold, namely a classical mechanical system with linear
dissipation and its relativistic extension, and propose discretizations based on conformal symplectic
integrators which preserve this underlying symplectic geometry. We argue that conformal symplectic
integrators can preserve convergence rates of the continuous system up to a negligible error. As
a surprising consequence of our construction, we show that the well-known and widely used classical
momentum method is a symplectic integrator, while the popular Nesterov's accelerated gradient
is not. Moreover, we introduce a relativistic generalization of classical momentum, called relativistic
gradient descent, which is symplectic, includes normalization of the momentum, and may result
in more stable/faster optimization for some problems. 