We propose a new family of inexact sequential quadratic approximation (SQA) methods, which we call
the inexact regularized proximal Newton ($\textsf{IRPN}$) method, for minimizing the sum of two
closed proper convex functions, one of which is smooth and the other is possibly non-smooth. Our
proposed method features strong convergence guarantees even when applied to problems with degenerate
solutions while allowing the inner minimization to be solved inexactly. Specifically, we prove
that when the problem possesses the so-called Luo-Tseng error bound (EB) property, $\textsf{IRPN}$
converges globally to an optimal solution, and the local convergence rate of the sequence of iterates
generated by $\textsf{IRPN}$ is linear, superlinear, or even quadratic, depending on the choice
of parameters of the algorithm. Prior to this work, such EB property has been extensively used to
establish the linear convergence of various first-order methods. However, to the best of our knowledge,
this work is the first to use the Luo-Tseng EB property to establish the superlinear convergence
of SQA-type methods for non-smooth convex minimization. As a consequence of our result, $\textsf{IRPN}$
is capable of solving regularized regression or classification problems under the high-dimensional
setting with provable convergence guarantees. We compare our proposed $\textsf{IRPN}$ with several
empirically efficient algorithms by applying them to the $\ell_1$-regularized logistic regression
problem. Experiment results show the competitiveness of our proposed method. 