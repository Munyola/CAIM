Many convex optimization methods are conceived of and analyzed in a largely separate fashion. In
contrast to this traditional separation, this manuscript points out and demonstrates the utility
of an important but largely unremarked common thread running through many prominent optimization
methods. Specifically, we show that methods such as successive orthogonal projection, gradient
descent, projected gradient descent, the proximal point method, forward-backward splitting,
the alternating direction method of multipliers, and under- or over-relaxed variants of the preceding
all involve updates that are of a common type --- namely, the updates satisfy a property known as pseudocontractivity.
Moreover, since the property of pseudocontractivity is preserved under both composition and convex
combination, updates constructed via these operations from pseudocontractive updates are themselves
pseudocontractive. Having demonstrated that pseudocontractive updates are to be found in many
optimization methods, we then provide an initial example of the type of unified analysis that becomes
possible in the settings where the property of pseudocontractivity is found to hold. Specifically,
we prove a novel bound satisfied by the norm of the difference in iterates of pseudocontractive updates
and we then use this bound to establish an $\mathcal{O}(1/N)$ worst-case convergence rate on the
error criterion $\left\Vert x^{k}-x^{k+1}\right\Vert ^{2} $ for any method involving pseudocontractive
updates (where $N$ is the number of iterations). 