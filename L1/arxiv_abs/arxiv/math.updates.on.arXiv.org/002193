In this paper, we consider a class of possibly nonconvex, nonsmooth and non-Lipschitz optimization
problems arising in many contemporary applications such as machine learning, variable selection
and image processing. To solve this class of problems, we propose a proximal gradient method with
extrapolation and line search (PGels). This method is developed based on a special potential function
and successfully incorporates both extrapolation and non-monotone line search, which are two
simple and efficient accelerating techniques for the proximal gradient method. Thanks to the line
search, this method allows more flexibilities in choosing the extrapolation parameters and updates
them adaptively at each iteration if a certain line search criterion is not satisfied. Moreover,
with proper choices of parameters, our PGels reduces to many existing algorithms. We also show that,
under some mild conditions, our line search criterion is well defined and any cluster point of the
sequence generated by PGels is a stationary point of our problem. In addition, by assuming the Kurdyka-${\L}$ojasiewicz
exponent of the objective in our problem, we further analyze the local convergence rate of two special
cases of PGels, including the widely used non-monotone proximal gradient method as one case. Finally,
we conduct some numerical experiments for solving the $\ell_1$ regularized logistic regression
problem and the $\ell_{1\text{-}2}$ regularized least squares problem. Our numerical results
illustrate the efficiency of PGels and show the potential advantage of combining two accelerating
techniques. 