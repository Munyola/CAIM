We focus on the distribution regression problem: regressing to a real-valued response from a probability
distribution. Although there exist a large number of similarity measures between distributions,
very little is known about their generalization performance in specific learning tasks. Learning
problems formulated on distributions have an inherent two-stage sampled difficulty: in practice
only samples from sampled distributions are observable, and one has to build an estimate on similarities
computed between sets of points. To the best of our knowledge, the only existing method with consistency
guarantees for distribution regression requires kernel density estimation as an intermediate
step (which suffers from slow convergence issues in high dimensions), and the domain of the distributions
to be compact Euclidean. In this paper, we provide theoretical guarantees for a remarkably simple
algorithmic solution to the distribution regression problem: embed the distributions to a reproducing
kernel Hilbert space, and learn a ridge regressor from the embeddings to the outputs. Our main contribution
is to prove the consistency of this technique in the two-stage sampled setting under mild conditions
(on separable, topological domains endowed with kernels). As a special case, we establish the consistency
of the classical set kernel [Haussler, 1999; Gartner et. al, 2002] in regression (a 15-year-old
open question), and cover more recent kernels on distributions, including those due to [Christmann
and Steinwart, 2010]. 