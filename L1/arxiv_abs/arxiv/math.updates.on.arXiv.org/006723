We study the policy evaluation problem in multi-agent reinforcement learning, where a group of
agents operates in a common environment. In this problem, the goal of the agents is to cooperatively
evaluate the global discounted accumulative reward, which is composed of local rewards observed
by the agents. Over a series of time steps, the agents act, get rewarded, update their local estimate
of the value function, then communicate with their neighbors. The local update at each agent can
be interpreted as a distributed variant of the popular temporal difference learning methods TD$(\lambda)$.
Our main contribution is to provide a finite-analysis on the performance of this distributed TD$(\lambda)$
for both constant and time-varying step sizes. The key idea in our analysis is to utilize the geometric
mixing time $\tau$ of the underlying Markov chain, that is, although the "noise" in our algorithm
is Markovian, their dependence is almost weakened out every $\tau$ step. In particular, we provide
an explicit formula for the upper bound on the rates of the proposed method as a function of the network
topology, the discount factor, the constant $\lambda$, and the mixing time $\tau$. Our results
theoretically address some numerical observations of TD$(\lambda)$, that is, $\lambda=1$ gives
the best approximation of the function values while $\lambda = 0$ leads to better performance when
there is a large variance in the algorithm. Our results complement the existing literature, where
such an explicit formula for the rates of distributed TD$(\lambda)$ is not available. 