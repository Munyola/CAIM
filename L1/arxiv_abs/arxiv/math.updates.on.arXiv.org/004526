Fano's inequality is one of the most elementary and highly important tools in information theory.
This study investigates generalization of Fano's inequality in the following four ways: (i) the
alphabet $\mathcal{X}$ of a random variable $X$ is \emph{countably infinite;} (ii) the induced
probability distribution $P_{X}$ is fixed to a given $\mathcal{X}$-marginal $Q$; (iii) Fano's
inequality is established on a general conditional information $\mathfrak{h}_{\phi}(X \mid
Y)$; and (iv) the decoding rule is generalized from a unique-decoding to a list-decoding scheme.
In other words, our Fano-type inequalities are tight upper bounds on $\mathfrak{h}_{\phi}(X \mid
Y)$ subject to a tolerated list-decoding error probability and a fixed $\mathcal{X}$-marginal
$P_{X} = Q$. Since $\mathfrak{h}_{\phi}(X \mid Y)$ is a general definition without explicit form,
our Fano-type inequalities give us some insights how to measure conditional information. Moreover,
since $\mathfrak{h}_{\phi}(X \mid Y)$ can be reduced to the conditional Shannon and R\'{e}nyi's
entropies, our Fano-type inequalities can be directly reduced to some known results of generalizing
Fano's inequality. As an application of our Fano-type inequalities, we investigate various sufficient
conditions on a general source in which vanishing error probability implies vanishing equivocation,
which is one of the most important observations of Fano's inequality. Then, a novel characterization
of the asymptotic equipartition property (AEP) is given via our Fano-type inequalities. 