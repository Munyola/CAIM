We develop a theoretical framework for the frequentist assessment of Bayesian model selection,
specifically its ability to select the (Kullback-Leibler) optimal model and to portray the corresponding
uncertainty. The contribution is not proving consistency for a specific prior, but giving a general
strategy for such proofs. Its basis applies to any model, prior, sample size, parameter dimensionality
and (although only briefly exploited here) under model misspecification. As an immediate consequence
the framework also characterizes a strong form of convergence for $L_0$ penalties and associated
pseudo-posterior probabilities of potential interest for uncertainty quantification. The main
advantage of the framework is that, instead of studying complex high-dimensional stochastic sums,
it suffices to bound certain Bayes factor tails and use standard tools to determine the convergence
of deterministic series. As a second contribution we deploy the framework to canonical linear regression.
These findings give a high-level description of when one can achieve consistency and at what rate
for a wide class of priors as a function of the data-generating truth, sample size and dimensionality.
They also indicate when it is possible to use less sparse priors to improve inherent sparsity vs.
power trade-offs that are not adequately captured by studying asymptotically optimal rates. Our
empirical illustrations align with these findings, underlining the importance of considering
the problem at hand's characteristics to judge the quality of model selection procedures, rather
than relying purely on asymptotics. 