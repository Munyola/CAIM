We consider the general problem of minimizing an objective function which is the sum of a convex function
(not strictly convex) and absolute values of a subset of variables (or equivalently the l1-norm
of the variables). This problem appears extensively in modern statistical applications associated
with high-dimensional data or "big data", and correspond to optimizing l1-regularized likelihoods
in the context of model selection. In such applications, cyclic coordinatewise minimization,
where the objective function is sequentially minimized with respect to each individual coordinate,
is often employed as it offers a computationally cheap and effective method. Consequently, it is
crucial to obtain theortical guarantees of convergence for the cyclic coordinatewise minimization
in this setting. Previous results in the literature only establish either, (i) that every limit
point of the sequence of iterates is a stationary point of the objective function, or (ii) establish
convergence under special assumptions, or (iii) establish convergence for a different minimization
approach, which uses quadratic approximation based gradient descent followed by an inexact line
search. In this paper, a rigorous general proof of convergence for the cyclic coordinatewise minimization
algorithm is provided. We demonstrate the the usefulness of our general results in contemporary
applications by employing them to prove convergence of two algorithms commonly used in high-dimensional
covariance estimation and logistic regression 