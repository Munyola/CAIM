The problem of numerical differentiation can be thought of as an inverse problem by considering
it as solving a Volterra equation. It is well known that such inverse integral problems are ill-posed
and one requires regularization methods to approximate the solution. The commonly practiced regularization
methods are parameter based like Tikhonov regularization, which have certain inherent difficulties
like choosing an optimal value of the regularization parameter to balance between the fitting and
smoothing terms, which is a non-trivial task. Hence the solution recovered is sometimes either
over-fitted or over-smoothed, especially in the presence of discontinuities. In this paper, we
propose a regularization method that approximates the solution without depending on any external
parameters and thus avoids the necessity of calculating an optimal parameter choice. We construct
a minimizing functional that converts the ill-posed problem to a conditionally well-posed problem,
independent of any parameters. To demonstrate the effectiveness of the new method we provide some
examples comparing the numerical results obtained from our method with results obtained from some
of the popular regularization methods such as Tikhonov regularization, total variation, smoothing
spline and the polynomial regression method. We also provide an effective heuristic stopping criteria
when the error norm is not known and hence the method is very effective for real life data where we don't
expect any knowledge on the error involved. 