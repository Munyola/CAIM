This article offers a simplified approach to the distribution theory of randomly weighted averages
or $P$-means $M_P(X):= \sum_{j} X_j P_j$, for a sequence of i.i.d.random variables $X, X_1, X_2,
\ldots$, and independent random weights $P:= (P_j)$ with $P_j \ge 0$ and $\sum_{j} P_j = 1$. The collection
of distributions of $M_P(X)$, indexed by distributions of $X$, is shown to encode Kingman's partition
structure derived from $P$. For instance, if $X_p$ has Bernoulli$(p)$ distribution on $\{0,1\}$,
the $n$th moment of $M_P(X_p)$ is a polynomial function of $p$ which equals the probability generating
function of the number $K_n$ of distinct values in a sample of size $n$ from $P$: $E (M_P(X_p))^n =
E p^{K_n}$. This elementary identity illustrates a general moment formula for $P$-means in terms
of the partition structure associated with random samples from $P$, first developed by Diaconis
and Kemperman (1996) and Kerov (1998) in terms of random permutations. As shown by Tsilevich (1997)
if the partition probabilities factorize in a way characteristic of the generalized Ewens sampling
formula with two parameters $(\alpha,\theta)$, found by Pitman (1992), then the moment formula
yields the Cauchy-Stieltjes transform of an $(\alpha,\theta)$ mean. The analysis of these random
means includes the characterization of $(0,\theta)$-means, known as Dirichlet means, due to Von
Neumann (1941), Watson (1956) and Cifarelli and Regazzini (1990) and generalizations of L\'evy's
arcsine law for the time spent positive by a Brownian motion, due to Darling (1949) Lamperti (1958)
and Barlow, Pitman and Yor (1989). 