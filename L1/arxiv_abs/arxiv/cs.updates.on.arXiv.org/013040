Deep networks are an integral part of the current machine learning paradigm. Their inherent ability
to learn complex functional mappings between data and various target variables, while discovering
hidden, task-driven features, makes them a powerful technology in a wide variety of applications.
Nonetheless, the success of these networks typically relies on the availability of sufficient
training data to optimize a large number of free parameters while avoiding overfitting, especially
for networks with large capacity. In scenarios with limited training budgets, e.g., supervised
tasks with limited labeled samples, several generic and/or task-specific regularization techniques,
including data augmentation, have been applied to improve the generalization of deep networks.Typically
such regularizations are introduced independently of that data or training scenario, and must
therefore be tuned, tested, and modified to meet the needs of a particular network. In this paper,
we propose a novel regularization framework that is driven by the population-level statistics
of the feature space to be learned. The regularization is in the form of a \textbf{cooperating subnetwork},
which is an auto-encoder architecture attached to the feature space and trained in conjunction
with the primary network. We introduce the architecture and training methodology and demonstrate
the effectiveness of the proposed cooperative network-based regularization in a variety of tasks
and architectures from the literature. Our code is freely available at \url{https://github.com/riddhishb/CoopSubNet
