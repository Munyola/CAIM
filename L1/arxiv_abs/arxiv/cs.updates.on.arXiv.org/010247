In image quality enhancement processing, it is the most important to predict how humans perceive
processed images since human observers are the ultimate receivers of the images. Thus, objective
image quality assessment (IQA) methods based on human visual sensitivity from psychophysical
experiments have been extensively studied. Thanks to the powerfulness of deep convolutional neural
networks (CNN), many CNN based IQA models have been studied. However, previous CNN-based IQA models
have not fully utilized the characteristics of human visual systems (HVS) for IQA problems by simply
entrusting everything to CNN where the CNN-based models are often trained as a regressor to predict
the scores of subjective quality assessment obtained from IQA datasets. In this paper, we propose
a novel HVS-inspired deep IQA network, called Deep HVS-IQA Net, where the human psychophysical
characteristics such as visual saliency and just noticeable difference (JND) are incorporated
at the front-end of the Deep HVS-IQA Net. To our best knowledge, our work is the first HVS-inspired
trainable IQA network that considers both the visual saliency and JND characteristics of HVS. Furthermore,
we propose a rank loss to train our Deep HVS-IQA Net effectively so that perceptually important features
can be extracted for image quality prediction. The rank loss can penalize the Deep HVS-IQA Net when
the order of its predicted quality scores is different from that of the ground truth scores. We evaluate
the proposed Deep HVS-IQA Net on large IQA datasets where it outperforms all the recent state-of-the-art
IQA methods. 