A neural network deployed in the wild may be asked to make predictions for inputs that were drawn from
a different distribution than that of the training data. A plethora of work has demonstrated that
it is easy to find or synthesize inputs for which a neural network is highly confident yet wrong. Generative
models are widely viewed to be robust to such mistaken confidence as modeling the density of the input
features can be used to detect novel, out-of-distribution inputs. In this paper we challenge this
assumption. We find that the model density from flow-based models, VAEs and PixelCNN cannot distinguish
images of common objects such as dogs, trucks, and horses (i.e. CIFAR-10) from those of house numbers
(i.e. SVHN), assigning a higher likelihood to the latter when the model is trained on the former.
We focus our analysis on flow-based generative models in particular since they are trained and evaluated
via the exact marginal likelihood. We find such behavior persists even when we restrict the flow
models to constant-volume transformations. These transformations admit some theoretical analysis,
and we show that the difference in likelihoods can be explained by the location and variances of the
data and the model curvature, which shows that such behavior is more general and not just restricted
to the pairs of datasets used in our experiments. Our results caution against using the density estimates
from deep generative models to identify inputs similar to the training distribution, until their
behavior on out-of-distribution inputs is better understood. 