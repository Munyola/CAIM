X-ray image based surgical tool navigation is fast and supplies accurate images of deep seated structures.
Typically, recovering the 6 DOF rigid pose and deformation of tools with respect to the X-ray camera
can be accurately achieved through intensity-based 2D/3D registration of 3D images or models to
2D X-rays. However, the capture range of image-based 2D/3D registration is inconveniently small
suggesting that automatic and robust initialization strategies are of critical importance. This
manuscript describes a first step towards leveraging semantic information of the imaged object
to initialize 2D/3D registration within the capture range of image-based registration by performing
concurrent segmentation and localization of dexterous surgical tools in X-ray images. We presented
a learning-based strategy to simultaneously localize and segment dexterous surgical tools in
X-ray images and demonstrate promising performance on synthetic and ex vivo data. We currently
investigate methods to use semantic information extracted by the proposed network to reliably
and robustly initialize image-based 2D/3D registration. While image-based 2D/3D registration
has been an obvious focus of the CAI community, robust initialization thereof (albeit critical)
has largely been neglected. This manuscript discusses learning-based retrieval of semantic information
on imaged-objects as a stepping stone for such initialization and may therefore be of interest to
the IPCAI community. Since results are still preliminary and only focus on localization, we target
the Long Abstract category. 