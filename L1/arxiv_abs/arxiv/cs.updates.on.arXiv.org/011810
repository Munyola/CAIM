Over the past few years, Generative Adversarial Networks (GANs) have garnered increased interest
among researchers in Computer Vision, with applications including, but not limited to, image generation,
translation, imputation, and super-resolution. Nevertheless, no GAN-based method has been proposed
in the literature that can successfully represent, generate or translate 3D facial shapes (meshes).
This can be primarily attributed to two facts, namely that (a) publicly available 3D face databases
are scarce as well as limited in terms of sample size and variability (e.g., few subjects, little
diversity in race and gender), and (b) mesh convolutions for deep networks present several challenges
that are not entirely tackled in the literature, leading to operator approximations and model instability,
often failing to preserve high-frequency components of the distribution. As a result, linear methods
such as Principal Component Analysis (PCA) have been mainly utilized towards 3D shape analysis,
despite being unable to capture non-linearities and high frequency details of the 3D face - such
as eyelid and lip variations. In this work, we present 3DFaceGAN, the first GAN tailored towards
modeling the distribution of 3D facial surfaces, while retaining the high frequency details of
3D face shapes. We conduct an extensive series of both qualitative and quantitative experiments,
where the merits of 3DFaceGAN are clearly demonstrated against other, state-of-the-art methods
in tasks such as 3D shape representation, generation, and translation. 