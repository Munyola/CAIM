Feature generating networks face to the most important question, which is the fitting difference
(inconsistence) of the distribution between the generated feature and the real data. This inconsistence
further influence the performance of the networks model, because training samples from seen classes
is disjointed with testing samples from unseen classes in zero-shot learning (ZSL). In generalization
zero-shot learning (GZSL), testing samples come from not only seen classes but also unseen classes
for closer to the practical situation. Therefore, most of feature generating networks difficultly
obtain satisfactory performance for the challenging GZSL by adversarial learning the distribution
of semantic classes. To alleviate the negative influence of this inconsistence for ZSL and GZSL,
transfer feature generating networks with semantic classes structure (TFGNSCS) is proposed to
construct networks model for improving the performance of ZSL and GZSL. TFGNSCS can not only consider
the semantic structure relationship between seen and unseen classes, but also learn the difference
of generating features by transferring classification model information from seen to unseen classes
in networks. The proposed method can integrate the transfer loss, the classification loss and the
Wasserstein distance loss to generate enough CNN features, on which softmax classifiers are trained
for ZSL and GZSL. Experiments demonstrate that the performance of TFGNSCS outperforms that of the
state of the arts on four challenging datasets, which are CUB,FLO,SUN, AWA in GZSL. 