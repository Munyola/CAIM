Tool-use applications in robotics require conceptual knowledge about objects for informed decision
making and object interactions. State-of-the-art methods employ hand-crafted symbolic knowledge
which is defined from a human perspective and grounded into sensory data afterwards. However, due
to different sensing and acting capabilities of robots, their conceptual understanding of objects
must be generated from a robot's perspective entirely, which asks for robot-centric conceptual
knowledge about objects. With this goal in mind, this article motivates that such knowledge should
be based on physical and functional properties of objects. Consequently, a selection of ten properties
is defined and corresponding extraction methods are proposed. This multi-modal property extraction
forms the basis on which our second contribution, a robot-centric knowledge generation is build
on. It employs unsupervised clustering methods to transform numerical property data into symbols,
and Bivariate Joint Frequency Distributions and Sample Proportion to generate conceptual knowledge
about objects using the robot-centric symbols. A preliminary implementation of the proposed framework
is employed to acquire a dataset comprising physical and functional property data of 110 houshold
objects. This Robot-Centric dataSet (RoCS) is used to evaluate the framework regarding the property
extraction methods, the semantics of the considered properties within the dataset and its usefulness
in real-world applications such as tool substitution. 