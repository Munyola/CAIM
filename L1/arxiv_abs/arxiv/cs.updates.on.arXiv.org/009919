We propose a general test of conditional independence. The conditional predictive impact (CPI)
is a provably consistent and unbiased estimator of one or several features' association with a given
outcome, conditional on a (potentially empty) reduced feature set. The measure can be calculated
using any supervised learning algorithm and loss function. It relies on no parametric assumptions
and applies equally well to continuous and categorical predictors and outcomes. The CPI can be efficiently
computed for low- or high-dimensional data without any sparsity constraints. We illustrate PAC-Bayesian
convergence rates for the CPI and develop statistical inference procedures for evaluating its
magnitude, significance, and precision. These tests aid in feature and model selection, extending
traditional frequentist and Bayesian techniques to general supervised learning tasks. The CPI
may also be used in conjunction with causal discovery algorithms to identify underlying graph structures
for multivariate systems. We test our method in conjunction with various algorithms, including
linear regression, neural networks, random forests, and support vector machines. Empirical results
show that the CPI compares favorably to alternative variable importance measures and other nonparametric
tests of conditional independence on a diverse array of real and simulated datasets. Simulations
confirm that our inference procedures successfully control Type I error and achieve nominal coverage
probability. Our method has been implemented in an R package, cpi, which can be downloaded from https://github.com/dswatson/cpi.
