Two modalities are often used to convey information in a complementary and beneficial manner, e.g.,
in online news, videos, educational resources, or scientific publications. The automatic understanding
of semantic correlations between text and associated images as well as their interplay has a great
potential for enhanced multimodal web search and recommender systems. However, automatic understanding
of multimodal information is still an unsolved research problem. Recent approaches such as image
captioning focus on precisely describing visual content and translating it to text, but typically
address neither semantic interpretations nor the specific role or purpose of an image-text constellation.
In this paper, we go beyond previous work and investigate, inspired by research in visual communication,
useful semantic image-text relations for multimodal information retrieval. We derive a categorization
of eight semantic image-text classes (e.g., "illustration" or "anchorage") and show how they can
systematically be characterized by a set of three metrics: cross-modal mutual information, semantic
correlation, and the status relation of image and text. Furthermore, we present a deep learning
system to predict these classes by utilizing multimodal embeddings. To obtain a sufficiently large
amount of training data, we have automatically collected and augmented data from a variety of data
sets and web resources, which enables future research on this topic. Experimental results on a demanding
test set demonstrate the feasibility of the approach. 