Lack of annotated samples vastly restrains the direct application of deep learning supervised
method in remote sensing scene classification. Researchers try to tackle this issue with the aid
of unsupervised learning ability of generative adversarial networks (GANs). However, in these
works, the generated samples are only used inside the GANs for training, which haven't proved the
effectiveness of the GAN-generated samples using as augmentation data for training other deep
networks. Moreover, traditional image transformation operations such as flip and rotation, although
limited in quantity and diversity, are still broadly applied for data augmentation. Thus the question
whether the GAN-generated samples perform better than the transformed samples is necessary to
be answered. Therefore, we propose a SiftingGAN approach to generate more numerous, more diverse,
more authentic labeled samples for data augmentation. SiftingGAN extends traditional GAN framework
with an Online-Output method for sample generation, a Generative-Model-Sifting method for model
sifting, and a Labeled-Sample-Discriminating method for sample sifting. We conduct three groups
of control experiments by changing the data ratio of original/augmented and applying differently
augmented samples. The experimental results on AID dataset verify that the samples generated by
the proposed SiftingGAN effectively improve the scene classification baseline and perform better
than the samples produced by traditional geometric transformation operations. 