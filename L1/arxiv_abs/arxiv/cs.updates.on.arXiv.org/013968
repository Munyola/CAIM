Cyberbullying and cyberaggression are increasingly worrisome phenomena affecting people across
all demographics. More than half of young social media users worldwide have been exposed to such
prolonged and/or coordinated digital harassment. Victims can experience a wide range of emotions,
with negative consequences such as embarrassment, depression, isolation from other community
members, which embed the risk to lead to even more critical consequences, such as suicide attempts.
In this work, we take the first concrete steps to understand the characteristics of abusive behavior
in Twitter, one of today's largest social media platforms. We analyze 1.2 million users and 2.1 million
tweets, comparing users participating in discussions around seemingly normal topics like the
NBA, to those more likely to be hate-related, such as the Gamergate controversy, or the gender pay
inequality at the BBC station. We also explore specific manifestations of abusive behavior, i.e.,
cyberbullying and cyberaggression, in one of the hate-related communities (Gamergate). We present
a robust methodology to distinguish bullies and aggressors from normal Twitter users by considering
text, user, and network-based attributes. Using various state-of-the-art machine learning algorithms,
we classify these accounts with over 90% accuracy and AUC. Finally, we discuss the current status
of Twitter user accounts marked as abusive by our methodology, and study the performance of potential
mechanisms that can be used by Twitter to suspend users in the future. 