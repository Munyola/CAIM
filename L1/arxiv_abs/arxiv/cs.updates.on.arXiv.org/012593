This paper presents a novel framework for automatic speech-driven gesture generation, applicable
to human-agent interaction including both virtual agents and robots. Specifically, we extend
recent deep-learning-based, data-driven methods for speech-driven gesture generation by incorporating
representation learning. Our model takes speech as input and produces gestures as output, in the
form of a sequence of 3D coordinates. Our approach consists of two steps. First, we learn a lower-dimensional
representation of human motion using a denoising autoencoder neural network, consisting of a motion
encoder MotionE and a motion decoder MotionD. The learned representation preserves the most important
aspects of the human pose variation while removing less relevant variation. Second, we train a novel
encoder network SpeechE to map from speech to a corresponding motion representation with reduced
dimensionality. At test time, the speech encoder and the motion decoder networks are combined:
SpeechE predicts motion representations based on a given speech signal and MotionD then decodes
these representations to produce motion sequences. We evaluate different representation sizes
in order to find the most effective dimensionality for the representation. We also evaluate the
effects of using different speech features as input to the model. We find that mel-frequency cepstral
coefficients (MFCCs), alone or combined with prosodic features, perform the best. The results
of a subsequent user study confirm the benefits of the representation learning. 