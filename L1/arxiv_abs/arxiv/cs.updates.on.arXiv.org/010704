Saliency prediction can benefit from training that involves scene understanding that may be tangential
to the central task; this may include understanding places, spatial layout, objects or involve
different datasets and their bias. One can combine models, but to do this in a sophisticated manner
can be complex, and also result in unwieldy networks or produce competing objectives that are hard
to balance. In this paper, we propose a scalable system to leverage multiple powerful deep CNN models
to better extract visual features for saliency prediction. Our design differs from previous studies
in that the whole system is trained in an almost end-to-end piece-wise fashion. The encoder and decoder
components are separately trained to deal with complexity tied to the computational paradigm and
required space. Furthermore, the encoder can contain more than one CNN model to extract features,
and models can have different architectures or be pre-trained on different datasets. This parallel
design yields a better computational paradigm overcoming limits to the variety of information
or inference that can be combined at the encoder stage towards deeper networks and a more powerful
encoding. Our network can be easily expanded almost without any additional cost, and other pre-trained
CNN models can be incorporated availing a wider range of visual knowledge. We denote our expandable
multi-layer network as EML-NET and our method achieves the state-of-the-art results on the public
saliency benchmarks, SALICON, MIT300 and CAT2000. 