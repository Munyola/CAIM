The quest of `can machines think' and `can machines do what human do' are quests that drive the development
of artificial intelligence. Although recent artificial intelligence succeeds in many data intensive
applications, it still lacks the ability of learning from limited exemplars and fast generalizing
to new tasks. To tackle this problem, one has to turn to machine learning, which supports the scientific
study of artificial intelligence. Particularly, a machine learning problem called Few-Shot Learning
(FSL) targets at this case. It can rapidly generalize to new tasks of limited supervised experience
by turning to prior knowledge, which mimics human's ability to acquire knowledge from few examples
through generalization and analogy. It has been seen as a test-bed for real artificial intelligence,
a way to reduce laborious data gathering and computationally costly training, and antidote for
rare cases learning. With extensive works on FSL emerging, we give a comprehensive survey for it.
We first give the formal definition for FSL. Then we point out the core issues of FSL, which turns the
problem from "how to solve FSL" to "how to deal with the core issues". Accordingly, existing works
from the birth of FSL to the most recent published ones are categorized in a unified taxonomy, with
thorough discussion of the pros and cons for different categories. Finally, we envision possible
future directions for FSL in terms of problem setup, techniques, applications and theory, hoping
to provide insights to both beginners and experienced researchers. 