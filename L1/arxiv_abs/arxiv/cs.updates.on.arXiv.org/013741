Quantum Neural Networks (QNNs) are a promising variational learning paradigm with applications
to near-term quantum processors, however they still face some significant challenges. One such
challenge is finding good parameter initialization heuristics that ensure rapid and consistent
convergence to local minima of the parameterized quantum circuit landscape. In this work, we train
classical neural networks to assist in the quantum learning process, also know as meta-learning,
to rapidly find approximate optima in the parameter landscape for several classes of quantum variational
algorithms. Specifically, we train classical recurrent neural networks to find approximately
optimal parameters within a small number of queries of the cost function for the Quantum Approximate
Optimization Algorithm (QAOA) for MaxCut, QAOA for Sherrington-Kirkpatrick Ising model, and
for a Variational Quantum Eigensolver for the Hubbard model. By initializing other optimizers
at parameter values suggested by the classical neural network, we demonstrate a significant improvement
in the total number of optimization iterations required to reach a given accuracy. We further demonstrate
that the optimization strategies learned by the neural network generalize well across a range of
problem instance sizes. This opens up the possibility of training on small, classically simulatable
problem instances, in order to initialize larger, classically intractably simulatable problem
instances on quantum devices, thereby significantly reducing the number of required quantum-classical
optimization iterations. 