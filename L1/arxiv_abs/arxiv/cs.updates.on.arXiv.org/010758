Gaussian processes (GP) are a popular Bayesian approach for the optimization of black-box functions.
Despite their effectiveness in simple problems, GP-based algorithms hardly scale to complex high-dimensional
functions, as their per-iteration time and space cost is at least quadratic in the number of dimensions
$d$ and iterations $t$. Given a set of $A$ alternative to choose from, the overall runtime $O(t^3A)$
quickly becomes prohibitive. In this paper, we introduce BKB (budgeted kernelized bandit), a novel
approximate GP algorithm for optimization under bandit feedback that achieves near-optimal regret
(and hence near-optimal convergence rate) with near-constant per-iteration complexity and no
assumption on the input space or covariance of the GP. Combining a kernelized linear bandit algorithm
(GP-UCB) with randomized matrix sketching technique (i.e., leverage score sampling), we prove
that selecting inducing points based on their posterior variance gives an accurate low-rank approximation
of the GP, preserving variance estimates and confidence intervals. As a consequence, BKB does not
suffer from variance starvation, an important problem faced by many previous sparse GP approximations.
Moreover, we show that our procedure selects at most $\tilde{O}(d_{eff})$ points, where $d_{eff}$
is the effective dimension of the explored space, which is typically much smaller than both $d$ and
$t$. This greatly reduces the dimensionality of the problem, thus leading to a $O(TAd_{eff}^2)$
runtime and $O(A d_{eff})$ space complexity. 