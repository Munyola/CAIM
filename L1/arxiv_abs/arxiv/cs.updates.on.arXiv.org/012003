This paper presents a brand new nonparametric density estimation strategy named the best-scored
random forest density estimation whose effectiveness is supported by both solid theoretical analysis
and significant experimental performance. The terminology best-scored stands for selecting
one density tree with the best estimation performance out of a certain number of purely random density
tree candidates and we then name the selected one the best-scored random density tree. In this manner,
the ensemble of these selected trees that is the best-scored random density forest can achieve even
better estimation results than simply integrating trees without selection. From the theoretical
perspective, by decomposing the error term into two, we are able to carry out the following analysis:
First of all, we establish the consistency of the best-scored random density trees under $L_1$-norm.
Secondly, we provide the convergence rates of them under $L_1$-norm concerning with three different
tail assumptions, respectively. Thirdly, the convergence rates under $L_{\infty}$-norm is presented.
Last but not least, we also achieve the above convergence rates analysis for the best-scored random
density forest. When conducting comparative experiments with other state-of-the-art density
estimation approaches on both synthetic and real data sets, it turns out that our algorithm has not
only significant advantages in terms of estimation accuracy over other methods, but also stronger
resistance to the curse of dimensionality. 