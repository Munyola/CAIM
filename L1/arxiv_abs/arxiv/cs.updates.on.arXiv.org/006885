Optimizing for long term value is desirable in many practical applications, e.g. recommender systems.
The most common approach for long term value optimization is supervised learning using long term
value as the target. Unfortunately, long term metrics take a long time to measure (e.g., will customers
finish reading an ebook?), and vanilla forecasters cannot learn from examples until the outcome
is observed. In practical systems where new items arrive frequently, such delay can increase the
training-serving skew, thereby negatively affecting the model's predictions for new products.
We argue that intermediate observations (e.g., if customers read a third of the book in 24 hours)
can improve a model's predictions. We formalize the problem as a semi-stochastic model, where instances
are selected by an adversary but, given an instance, the intermediate observation and the outcome
are sampled from a factored joint distribution. We propose an algorithm that exploits intermediate
observations and theoretically quantify how much it can outperform any prediction method that
ignores the intermediate observations. Motivated by the theoretical analysis, we propose two
neural network architectures: Factored Forecaster (FF) which is ideal if our assumptions are satisfied,
and Residual Factored Forecaster (RFF) that is more robust to model mis-specification. Experiments
on two real world datasets, a dataset derived from GitHub repositories and another dataset from
a popular marketplace, show that RFF outperforms both FF as well as an algorithm that ignores intermediate
observations. 