Object detectors have witnessed great progress in recent years and have been widely deployed in
various important real-world scenarios, such as autonomous driving and face recognition. Therefore,
it is increasingly vital to investigate the vulnerability of modern object detectors to different
types of attacks. In this work, we demonstrate that actually many mainstream detectors (e.g. Faster
R-CNN) can be hacked by a tiny adversarial patch. It is a non-trivial task since the original adversarial
patch method can only be applied to image-level classifiers and is not capable to deal with the region
proposals involved in modern detectors. Instead, here we iteratively evolve a tiny patch inside
the input image so that it invalidates both proposal generation and the subsequent region classification
of Faster R-CNN, resulting in a successful attack. Specifically, the proposed adversarial patch
(namely, AdvDetPatch) can be trained toward any targeted class so that all the objects in any region
of the scene will be classified as that targeted class. One interesting observation is that the efficiency
of AdvDetPatch is not influenced by its location: no matter where it resides, the patch can always
invalidate RCNN after the same amount of iterations. Furthermore, we find that different target
classes have different degrees of vulnerability; and an AdvDetPatch with a larger size can perform
the attack more effectively. Extensive experiments show that our AdvDetPatch can reduce the mAP
of a state-of-the-art detector on PASCAL VOC 2012 from 71% to 25% and below. 