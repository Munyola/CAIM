Localization is a critical capability for robots, drones and autonomous vehicles operating in
a wide range of environments. One of the critical considerations for designing, training or calibrating
visual localization systems is the coverage of the visual sensors equipped on the platforms. In
an aerial context for example, the altitude of the platform and camera field of view plays a critical
role in how much of the environment a downward facing camera can perceive at any one time. Furthermore,
in other applications, such as on roads or in indoor environments, additional factors such as camera
resolution and sensor placement altitude can also affect this coverage. The sensor coverage and
the subsequent processing of its data also has significant computational implications. In this
paper we present for the first time a set of methods for automatically determining the trade-off
between coverage and visual localization performance, enabling the identification of the minimum
visual sensor coverage required to obtain optimal localization performance with minimal compute.
We develop a localization performance indicator based on the overlapping coefficient, and demonstrate
its predictive power for localization performance with a certain sensor coverage. We evaluate
our method on several challenging real-world datasets from aerial and ground-based domains, and
demonstrate that our method is able to automatically optimize for coverage using a small amount
of calibration data. We hope these results will assist in the design of localization systems for
future autonomous robot, vehicle and flying systems. 