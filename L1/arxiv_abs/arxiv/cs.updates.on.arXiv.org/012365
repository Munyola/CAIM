We propose an AdaPtive Noise Augmentation (PANDA) technique to regularize the estimation and construction
of undirected graphical models. PANDA iteratively optimizes the objective function given the
noise augmented data until convergence to achieve regularization on model parameters. The augmented
noises can be designed to achieve various regularization effects on graph estimation, such as the
bridge (including lasso and ridge), elastic net, adaptive lasso, and SCAD penalization; it also
realizes the group lasso and fused ridge. We examine the tail bound of the noise-augmented loss function
and establish that the noise-augmented loss function and its minimizer converge almost surely
to the expected penalized loss function and its minimizer, respectively. We derive the asymptotic
distributions for the regularized parameters through PANDA in generalized linear models, based
on which, inferences for the parameters can be obtained simultaneously with variable selection.
We show the non-inferior performance of PANDA in constructing graphs of different types in simulation
studies and apply PANDA to an autism spectrum disorder data to construct a mixed-node graph. We also
show that the inferences based on the asymptotic distribution of regularized parameter estimates
via PANDA achieve nominal or near-nominal coverage and are far more efficient, compared to some
existing post-selection procedures. Computationally, PANDA can be easily programmed in software
that implements (GLMs) without resorting to complicated optimization techniques. 