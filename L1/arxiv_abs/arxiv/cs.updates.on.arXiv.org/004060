Bike sharing provides an environment-friendly way for traveling and is booming all over the world.
Yet, due to the high similarity of user travel patterns, the bike imbalance problem constantly occurs,
especially for dockless bike sharing systems, causing significant impact on service quality and
company revenue. Thus, it has become a critical task for bike sharing systems to resolve such imbalance
efficiently. We model this problem as a Markov decision process (MDP), which takes both temporal
and spatial features into consideration. We propose a novel deep reinforcement learning algorithm
called Loss-Reduced Reinforcement Pricing (LRP), which builds upon the deterministic policy
gradient algorithm. Different from existing methods that often ignore spatial information and
rely heavily on accurate prediction, LRP is embedded with a novel network architecture to incorporate
the dependence of neighboring regions, for reducing the training loss in Q-function learning.
We conduct extensive experiments to evaluate the performance of the LRP algorithm, based on trajectory
data from Mobike, a major Chinese dockless bike sharing company. Results show that LRP performs
close to the 24-timeslot look-ahead optimization, and outperforms state-of-the-art methods
in both service level and bike distribution. It also transfers well when applied to unseen areas,
and can even make additional profit with the given budget. We further propose the first hybrid rebalancing
system, which take advantages of both the truck-based and user-based approaches, and outperforms
each individual approach. 