Inspired by the success of deploying deep learning in the fields of Computer Vision and Natural Language
Processing, this learning paradigm has also found its way into the field of Music Information Retrieval.
In order to benefit from deep learning in an effective, but also efficient manner, deep transfer
learning has become a common approach. In this approach, it is possible to reuse the output of a pre-trained
neural network as the basis for a new, yet unseen learning task. The underlying hypothesis is that
if the initial and new learning tasks show commonalities and are applied to the same type of data (e.g.
music audio), the generated deep representation of the data is also informative for the new task.
Since, however, most of the networks used to generate deep representations are trained using a single
initial learning task, the validity of the above hypothesis is questionable for an arbitrary new
learning task. In this paper we present the results of our investigation of what the best ways are
to generate deep representations for the data and learning tasks in the music domain. We conducted
this investigation via an extensive empirical study that involves multiple learning tasks, as
well as multiple deep learning architectures with varying levels of information sharing between
tasks, in order to learn music representations. We then validate these representations considering
multiple unseen learning tasks for evaluation. The results of our experiments yield several insights
on how to approach the design of methods for learning widely deployable deep data representations
in the music domain. 