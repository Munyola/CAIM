Bayesian Inference (BI) uses the Bayes' posterior whereas Logical Bayesian Inference (LBI) uses
the truth function or membership function as the inference tool. LBI was proposed because BI was
not compatible with the classical Bayes' prediction and didn't use logical probability and hence
couldn't express semantic meaning. In LBI, statistical probability and logical probability are
strictly distinguished, used at the same time, and linked by the third kind of Bayes' Theorem. The
Shannon channel consists of a set of transition probability functions whereas the semantic channel
consists of a set of truth functions. When a sample is large enough, we can directly derive the semantic
channel from Shannon's channel. Otherwise, we can use parameters to construct truth functions
and use the Maximum Semantic Information (MSI) criterion to optimize the truth functions. The MSI
criterion is equivalent to the Maximum Likelihood (ML) criterion, and compatible with the Regularized
Least Square (RLS) criterion. By matching the two channels one with another, we can obtain the Channels'
Matching (CM) algorithm. This algorithm can improve multi-label classifications, maximum likelihood
estimations (including unseen instance classifications), and mixture models. In comparison
with BI, LBI 1) uses the prior P(X) of X instead of that of Y or {\theta} and fits cases where the source
P(X) changes, 2) can be used to solve the denotations of labels, and 3) is more compatible with the
classical Bayes' prediction and likelihood method. LBI also provides a confirmation measure between
-1 and 1 for induction. 