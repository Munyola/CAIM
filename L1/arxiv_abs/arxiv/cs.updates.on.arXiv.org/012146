Current advances in Artificial Intelligence and machine learning in general, and deep learning
in particular have reached unprecedented impact not only across research communities, but also
over popular media channels. However, concerns about interpretability and accountability of
AI have been raised by influential thinkers. In spite of the recent impact of AI, several works have
identified the need for principled knowledge representation and reasoning mechanisms integrated
with deep learning-based systems to provide sound and explainable models for such systems. Neural-symbolic
computing aims at integrating, as foreseen by Valiant, two most fundamental cognitive abilities:
the ability to learn from the environment, and the ability to reason from what has been learned. Neural-symbolic
computing has been an active topic of research for many years, reconciling the advantages of robust
learning in neural networks and reasoning and interpretability of symbolic representation. In
this paper, we survey recent accomplishments of neural-symbolic computing as a principled methodology
for integrated machine learning and reasoning. We illustrate the effectiveness of the approach
by outlining the main characteristics of the methodology: principled integration of neural learning
with symbolic knowledge representation and reasoning allowing for the construction of explainable
AI systems. The insights provided by neural-symbolic computing shed new light on the increasingly
prominent need for interpretable and accountable AI systems. 