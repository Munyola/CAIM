With the widespread use of AI systems and applications in our everyday lives, it is important to take
fairness issues into consideration while designing and engineering these types of systems. Such
systems can be used in many sensitive environments to make important and life-changing decisions;
thus, it is crucial to ensure that the decisions do not reflect discriminatory behavior toward certain
groups or populations. We have recently seen work in machine learning, natural language processing,
and deep learning that addresses such challenges in different subdomains. With the commercialization
of these systems, researchers are becoming aware of the biases that these applications can contain
and have attempted to address them. In this survey we investigated different real-world applications
that have shown biases in various ways, and we listed different sources of biases that can affect
AI applications. We then created a taxonomy for fairness definitions that machine learning researchers
have defined in order to avoid the existing bias in AI systems. In addition to that, we examined different
domains and subdomains in AI showing what researchers have observed with regard to unfair outcomes
in the state-of-the-art methods and how they have tried to address them. There are still many future
directions and solutions that can be taken to mitigate the problem of bias in AI systems. We are hoping
that this survey will motivate researchers to tackle these issues in the near future by observing
existing work in their respective fields. 