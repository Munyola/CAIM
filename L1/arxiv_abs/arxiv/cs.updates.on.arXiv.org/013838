Interpretable Machine Learning (IML) has become increasingly important in many applications,
such as autonomous cars and medical diagnosis, where explanations are preferred to help people
better understand how machine learning systems work and further enhance their trust towards systems.
Particularly in robotics, explanations from IML are significantly helpful in providing reasons
for those adverse and inscrutable actions, which could impair the safety and profit of the public.
However, due to the diversified scenarios and subjective nature of explanations, we rarely have
the ground truth for benchmark evaluation in IML on the quality of generated explanations. Having
a sense of explanation quality not only matters for quantifying system boundaries, but also helps
to realize the true benefits to human users in real-world applications. To benchmark evaluation
in IML, in this paper, we rigorously define the problem of evaluating explanations, and systematically
review the existing efforts. Specifically, we summarize three general aspects of explanation
(i.e., predictability, fidelity and persuasibility) with formal definitions, and respectively
review the representative methodologies for each of them under different tasks. Further, a unified
evaluation framework is designed according to the hierarchical needs from developers and end-users,
which could be easily adopted for different scenarios in practice. In the end, open problems are
discussed, and several limitations of current evaluation techniques are raised for future explorations.
