Surrogate models are used to reduce the burden of expensive-to-evaluate objective functions in
optimization. By creating models which map genomes to objective values, these models can estimate
the performance of unknown inputs, and so be used in place of expensive objective functions. Evolutionary
techniques such as genetic programming or neuroevolution commonly alter the structure of the genome
itself. A lack of consistency in the genotype is a fatal blow to data-driven modeling techniques:
interpolation between points is impossible without a common input space. However, while the dimensionality
of genotypes may differ across individuals, in many domains, such as controllers or classifiers,
the dimensionality of the input and output remains constant. In this work we leverage this insight
to embed differing neural networks into the same input space. To judge the difference between the
behavior of two neural networks, we give them both the same input sequence, and examine the difference
in output. This difference, the phenotypic distance, can then be used to situate these networks
into a common input space, allowing us to produce surrogate models which can predict the performance
of neural networks regardless of topology. In a robotic navigation task, we show that models trained
using this phenotypic embedding perform as well or better as those trained on the weight values of
a fixed topology neural network. We establish such phenotypic surrogate models as a promising and
flexible approach which enables surrogate modeling even for representations that undergo structural
changes. 