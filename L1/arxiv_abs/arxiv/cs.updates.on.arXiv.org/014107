Deep learning has been shown to achieve impressive results in several domains like computer vision
and natural language processing. Deep architectures are typically trained following a supervised
scheme and, therefore, they rely on the availability of a large amount of labeled training data to
effectively learn their parameters. Neuro-symbolic approaches have recently gained popularity
to inject prior knowledge into a deep learner without requiring it to induce this knowledge from
data. These approaches can potentially learn competitive solutions with a significant reduction
of the amount of supervised data. A large class of neuro-symbolic approaches is based on First-Order
Logic to represent prior knowledge, that is relaxed to a differentiable form using fuzzy logic.
This paper shows that the loss function expressing these neuro-symbolic learning tasks can be unambiguously
determined given the selection of a t-norm generator. When restricted to simple supervised learning,
the presented theoretical apparatus provides a clean justification to the popular cross-entropy
loss, that has been shown to provide faster convergence and to reduce the vanishing gradient problem
in very deep structures. One advantage of the proposed learning formulation is that it can be extended
to all the knowledge that can be represented by a neuro-symbolic method, and it allows the development
of a novel class of loss functions, that the experimental results show to lead to faster convergence
rates than other approaches previously proposed in the literature. 