The high computation and memory storage of large deep neural networks (DNNs) models pose intensive
challenges to the conventional Von-Neumann architecture, incurring substantial data movements
in the memory hierarchy. The memristor crossbar array has emerged as a promising solution to mitigate
the challenges and enable low-power acceleration of DNNs. Memristor-based weight pruning and
weight quantization have been seperately investigated and proven effectiveness in reducing area
and power consumption compared to the original DNN model. However, there has been no systematic
investigation of memristor-based neuromorphic computing (NC) systems considering both weight
pruning and weight quantization. In this paper, we propose an unified and systematic memristor-based
framework considering both structured weight pruning and weight quantization by incorporating
alternating direction method of multipliers (ADMM) into DNNs training. We consider hardware constraints
such as crossbar blocks pruning, conductance range, and mismatch between weight value and real
devices, to achieve high accuracy and low power and small area footprint. Our framework is mainly
integrated by three steps, i.e., memristor-based ADMM regularized optimization, masked mapping
and retraining. Experimental results show that our proposed framework achieves 29.81X (20.88X)
weight compression ratio, with 98.38% (96.96%) and 98.29% (97.47%) power and area reduction on
VGG-16 (ResNet-18) network where only have 0.5% (0.76%) accuracy loss, compared to the original
DNN models. We share our models at link this http URL 