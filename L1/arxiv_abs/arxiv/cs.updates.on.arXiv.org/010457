Wikipedia is playing an increasingly central role on the web,and the policies its contributors
follow when sourcing and fact-checking content affect million of readers. Among these core guiding
principles, verifiability policies have a particularly important role. Verifiability requires
that information included in a Wikipedia article be corroborated against reliable secondary sources.
Because of the manual labor needed to curate and fact-check Wikipedia at scale, however, its contents
do not always evenly comply with these policies. Citations (i.e. reference to external sources)
may not conform to verifiability requirements or may be missing altogether, potentially weakening
the reliability of specific topic areas of the free encyclopedia. In this paper, we aim to provide
an empirical characterization of the reasons why and how Wikipedia cites external sources to comply
with its own verifiability guidelines. First, we construct a taxonomy of reasons why inline citations
are required by collecting labeled data from editors of multiple Wikipedia language editions.
We then collect a large-scale crowdsourced dataset of Wikipedia sentences annotated with categories
derived from this taxonomy. Finally, we design and evaluate algorithmic models to determine if
a statement requires a citation, and to predict the citation reason based on our taxonomy. We evaluate
the robustness of such models across different classes of Wikipedia articles of varying quality,
as well as on an additional dataset of claims annotated for fact-checking purposes. 