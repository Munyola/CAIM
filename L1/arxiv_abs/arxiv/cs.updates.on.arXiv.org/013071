Recent works on implicit regularization have shown that gradient descent converges to the max-margin
direction for logistic regression with one-layer or multi-layer linear networks. In this paper,
we generalize this result to homogeneous neural networks, including fully-connected and convolutional
neural networks with ReLU or LeakyReLU activations. In particular, we study the gradient flow (gradient
descent with infinitesimal step size) optimizing the logistic loss or cross-entropy loss of any
homogeneous model (possibly non-smooth), and show that if the training loss decreases below a certain
threshold, then we can define a smoothed version of the normalized margin which increases over time.
We also formulate a natural constrained optimization problem related to margin maximization,
and prove that both the normalized margin and its smoothed version converge to the objective value
at a KKT point of the optimization problem. Furthermore, we extend the above results to a large family
of loss functions. We conduct several experiments to justify our theoretical finding on MNIST and
CIFAR-10 datasets. For gradient descent with constant learning rate, we observe that the normalized
margin indeed keeps increasing after the dataset is fitted, but the speed is very slow. However,
if we schedule the learning rate more carefully, we can observe a more rapid growth of the normalized
margin. Finally, as margin is closely related to robustness, we discuss potential benefits of training
longer for improving the robustness of the model. 