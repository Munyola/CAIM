The core problem in many Latent Variable Models, widely used in Unsupervised Learning is to find
a latent k-simplex K in Rd given perturbed points from it, many of which lie far outside the simplex.
This problem was stated in [2] as an open problem. We address this problem under two deterministic
assumptions which replace varied stochastic assumptions specific to relevant individual models.
Our first contribution is to show that the convex hull K' of the averages of all delta n sized subsets
of data points is close to K. We call this subset-smoothing. While K' can have exponentially many
vertices, it is easily seen to have a polynomial time Optimization Oracle which in fact runs in time
O(nnz(data)). This is the starting point for our algorithm. The algorithm is simple: it has k stages
in each of which we use the oracle to find maximum of a carefully chosen linear function over K'; the
optimal x is an approximation to a new vertex of K. The simplicity does not carry over to the proof of
correctness. The proof is involved and uses existing and new tools from Numerical Analysis, especially
angles between singular spaces of close-by matrices. However, the simplicity of the algorithm,
especially the fact the only way we use the data is to do matrix-vector products leads to the claimed
time bound. This matches the best known algorithms in the special cases and is better when the input
is sparse as indeed is the case in many applications. Our algorithm applies to many special cases,
including Topic Models, Approximate Non-negative Matrix factorization, Overlapping community
Detection and Clustering. 