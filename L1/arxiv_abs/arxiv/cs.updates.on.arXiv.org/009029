Generative models have proven to be an outstanding tool for representing high-dimensional probability
distributions and generating realistic looking images. A fundamental characteristic of generative
models is their ability to produce multi-modal outputs. However, while training, they are often
susceptible to mode collapse, which means that the model is limited in mapping the input noise to
only a few modes of the true data distribution. In this paper, we draw inspiration from Determinantal
Point Process (DPP) to devise a generative model that alleviates mode collapse while producing
higher quality samples. DPP is an elegant probabilistic measure used to model negative correlations
within a subset and hence quantify its diversity. We use DPP kernel to model the diversity in real
data as well as in synthetic data. Then, we devise a generation penalty term that encourages the generator
to synthesize data with a similar diversity to real data. In contrast to previous state-of-the-art
generative models that tend to use additional trainable parameters or complex training paradigms,
our method does not change the original training scheme. Embedded in an adversarial training and
variational autoencoder, our Generative DPP approach shows a consistent resistance to mode-collapse
on a wide-variety of synthetic data and natural image datasets including MNIST, CIFAR10, and CelebA,
while outperforming state-of-the-art methods for data-efficiency, convergence-time, and generation
quality. Our code is publicly available. 