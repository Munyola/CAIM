In applications of supervised learning applied to medical image segmentation, the need for large
amounts of labeled data typically goes unquestioned. In particular, in the case of brain anatomy
segmentation, hundreds or thousands of weakly-labeled volumes are often used as training data.
In this paper, we first observe that for many brain structures, a small number of training examples,
(n=9), weakly labeled using Freesurfer 6.0, plus simple data augmentation, suffice as training
data to achieve high performance, achieving an overall mean Dice coefficient of $0.84 \pm 0.12$
compared to Freesurfer over 28 brain structures in T1-weighted images of $\approx 4000$ 9-10 year-olds
from the Adolescent Brain Cognitive Development study. We then examine two varieties of heteroscedastic
network as a method for improving classification results. An existing proposal by Kendall and Gal,
which uses Monte-Carlo inference to learn to predict the variance of each prediction, yields an
overall mean Dice of $0.85 \pm 0.14$ and showed statistically significant improvements over 25
brain structures. Meanwhile a novel heteroscedastic network which directly learns the probability
that an example has been mislabeled yielded an overall mean Dice of $0.87 \pm 0.11$ and showed statistically
significant improvements over all but one of the brain structures considered. The loss function
associated to this network can be interpreted as performing a form of learned label smoothing, where
labels are only smoothed where they are judged to be uncertain. 