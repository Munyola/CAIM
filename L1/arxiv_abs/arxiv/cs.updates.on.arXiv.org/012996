CNNs, RNNs, GCNs, and CapsNets have shown significant insights in representation learning and
are widely used in various text mining tasks such as large-scale multi-label text classification.
However, most existing deep models for multi-label text classification consider either the non-consecutive
and long-distance semantics or the sequential semantics, but how to consider them both coherently
is less studied. In addition, most existing methods treat output labels as independent methods,
but ignore the hierarchical relations among them, leading to useful semantic information loss.
In this paper, we propose a novel hierarchical taxonomy-aware and attentional graph capsule recurrent
CNNs framework for large-scale multi-label text classification. Specifically, we first propose
to model each document as a word order preserved graph-of-words and normalize it as a corresponding
words-matrix representation which preserves both the non-consecutive, long-distance and local
sequential semantics. Then the words-matrix is input to the proposed attentional graph capsule
recurrent CNNs for more effectively learning the semantic features. To leverage the hierarchical
relations among the class labels, we propose a hierarchical taxonomy embedding method to learn
their representations, and define a novel weighted margin loss by incorporating the label representation
similarity. Extensive evaluations on three datasets show that our model significantly improves
the performance of large-scale multi-label text classification by comparing with state-of-the-art
approaches. 