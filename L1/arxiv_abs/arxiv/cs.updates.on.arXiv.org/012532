In this paper, we aim at improving human motion prediction during human-robot collaboration in
industrial facilities by exploiting contributions from both physical and physiological signals.
Improved human-machine collaboration could prove useful in several areas, while it is crucial
for interacting robots to understand human movement as soon as possible to avoid accidents and injuries.
In this perspective, we propose a novel human-robot interface capable to anticipate the user intention
while performing reaching movements on a working bench in order to plan the action of a collaborative
robot. The proposed interface can find many applications in the Industry 4.0 framework, where autonomous
and collaborative robots will be an essential part of innovative facilities. A motion intention
prediction and a motion direction prediction levels have been developed to improve detection speed
and accuracy. A Gaussian Mixture Model (GMM) has been trained with IMU and EMG data following an evidence
accumulation approach to predict reaching direction. Novel dynamic stopping criteria have been
proposed to flexibly adjust the trade-off between early anticipation and accuracy according to
the application. The output of the two predictors has been used as external inputs to a Finite State
Machine (FSM) to control the behaviour of a physical robot according to user's action or inaction.
Results show that our system outperforms previous methods, achieving a real-time classification
accuracy of $94.3\pm2.9\%$ after $160.0msec\pm80.0msec$ from movement onset. 