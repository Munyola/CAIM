When deploying autonomous agents in the real world, we need to think about effective ways of communicating
our objectives to them. Traditional skill learning has revolved around reinforcement and imitation
learning, each with their own constraints on the format and temporal distribution with which information
between the human and the agent is exchanged. In contrast, when humans communicate with each other,
they make use of a large vocabulary of informative behaviors, including non-verbal communication,
which help to disambiguate their message throughout learning. Communicating throughout learning
allows them to identify any missing information, whereas the large vocabulary of behaviors helps
with selecting appropriate behaviors for communicating the required information. In this paper,
we introduce a multi-agent training framework, which emerges physical information-communicating
behaviors. The agent is trained, on a variety of tasks, with another agent, who knows the task and
serves as a human surrogate. Our approach produces an agent that is capable of learning interactively
from a human user, without a set of explicit demonstrations or a reward function. We conduct user
experiments on object gathering tasks with pixel observations, and confirm that the trained agent
learns from the human and that the joint performance significantly exceeds the performance of the
human acting alone. Further, through a series of experiments, we demonstrate the emergence of a
variety of learning behaviors, including information-sharing, information-seeking, and question-answering.
