We address the issue of editing musical performance data, in particular MIDI files representing
human musical performances. Editing such sequences raises specific issues due to the ambiguous
nature of musical objects. The first source of ambiguity is that musicians naturally produce many
deviations from the metrical frame. These deviations may be intentional or subconscious, but they
play an important role in conveying the groove or feeling of a performance. Relations between musical
elements are also usually implicit, creating even more ambiguity. A note is in relation with the
surrounding notes in many possible ways: it can be part of a melodic pattern, it can also play a harmonic
role with the simultaneous notes, or be a pedal-tone. All these aspects play an essential role that
should be preserved, as much as possible, when editing musical sequences. In this paper, we contribute
specifically to the problem of editing non-quantized, metrical musical sequences represented
as MIDI files. We first list of number of problems caused by the use of naive edition operations applied
to performance data, using a motivating example. We then introduce a model, called Dancing MIDI,
based on 1) two desirable, well-defined properties for edit operations and 2) two well-defined
operations, Split and Concat, with an implementation. We show that our model formally satisfies
the two properties, and that it prevents most of the problems that occur with naive edit operations
on our motivating example, as well as on a real-world example using an automatic harmonizer. 