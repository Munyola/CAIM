Datasets are an essential component for training effective machine learning models. In particular,
surgical robotic datasets have been key to many advances in semi-autonomous surgeries, skill assessment,
and training. Simulated surgical environments can enhance the data collection process by making
it faster, simpler and cheaper than real systems. In addition, combining data from multiple robotic
domains can provide rich and diverse training data for transfer learning algorithms. In this paper,
we present the DESK (Dexterous Surgical Skill) dataset. It comprises a set of surgical robotic skills
collected during a surgical training task using three robotic platforms: the Taurus II robot, Taurus
II simulated robot, and the YuMi robot. This dataset was used to test the idea of transferring knowledge
across different domains (e.g. from Taurus to YuMi robot) for a surgical gesture classification
task with seven gestures. We explored three different scenarios: 1) No transfer, 2) Transfer from
simulated Taurus to real Taurus and 3) Transfer from Simulated Taurus to the YuMi robot. We conducted
extensive experiments with three supervised learning models and provided baselines in each of
these scenarios. Results show that using simulation data during training enhances the performance
on the real robot where limited real data is available. In particular, we obtained an accuracy of
55% on the real Taurus data using a model that is trained only on the simulator data. Furthermore,
we achieved an accuracy improvement of 34% when 3% of the real data is added into the training process.
