Spike-based communication between biological neurons is sparse and unreliable. This enables
the brain to process visual information from the eyes efficiently. Taking inspiration from biology,
artificial spiking neural networks coupled with silicon retinas attempt to model these computations.
Recent findings in machine learning allowed the derivation of a family of powerful synaptic plasticity
rules approximating backpropagation for spiking networks. Are these rules capable of processing
real-world visual sensory data? In this paper, we evaluate the performance of Event-Driven Random
Backpropagation (eRBP) at learning representations from event streams provided by a Dynamic Vision
Sensor (DVS). First, we show that eRBP matches state-of-the-art performance on DvsGesture with
the addition of a simple covert attention mechanism. By remapping visual receptive fields relatively
to the center of the motion, this attention mechanism provides translation invariance at low computational
cost compared to convolutions. Second, we successfully integrate eRBP in a real robotic setup,
where a robotic arm grasps objects with respect to detected visual affordances. In this setup, visual
information is actively sensed by a DVS mounted on a robotic head performing microsaccadic eye movements.
We show that our method quickly classifies affordances within 100ms after microsaccade onset,
comparable to human performance reported in behavioral study. Our results suggest that advances
in neuromorphic technology and plasticity rules enable the development of autonomous robots operating
at high speed and low energy budget. 