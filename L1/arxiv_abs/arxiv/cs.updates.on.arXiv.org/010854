Plant root research can provide a way to attain stress-tolerant crops that produce greater yield
in a diverse array of conditions. Phenotyping roots in soil is often challenging due to the roots
being difficult to access and the use of time consuming manual methods. Rhizotrons allow visual
inspection of root growth through transparent surfaces. Agronomists currently manually label
photographs of roots obtained from rhizotrons using a line-intersect method to obtain root length
density and rooting depth measurements which are essential for their experiments. We investigate
the effectiveness of an automated image segmentation method based on the U-Net Convolutional Neural
Network (CNN) architecture to enable such measurements. We design a data-set of 50 annotated Chicory
(Cichorium intybus L.) root images which we use to train, validate and test the system and compare
against a baseline built using the Frangi vesselness filter. We obtain metrics using manual annotations
and line-intersect counts. Our results on the held out data show our proposed automated segmentation
system to be a viable solution for detecting and quantifying roots. We evaluate our system using
867 images for which we have obtained line-intersect counts, attaining a Spearman rank correlation
of 0.9748 and an $r^2$ of 0.9217. We also achieve an $F_1$ of 0.7 when comparing the automated segmentation
to the manual annotations, with our automated segmentation system producing segmentations with
higher quality than the manual annotations for large portions of the image. 