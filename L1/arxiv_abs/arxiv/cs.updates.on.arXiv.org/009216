Binary data matrices can represent many types of data such as social networks, votes or gene expression.
In some cases, the analysis of binary matrices can be tackled with nonnegative matrix factorization
(NMF), where the observed data matrix is approximated by the product of two smaller nonnegative
matrices. In this context, probabilistic NMF assumes a generative model where the data is usually
Bernoulli-distributed. Often, a link function is used to map the factorization to the $[0,1]$ range,
ensuring a valid Bernoulli mean parameter. However, link functions have the potential disadvantage
to lead to uninterpretable models. Mean-parameterized NMF, on the contrary, overcomes this problem.
We propose a unified framework for Bayesian mean-parameterized nonnegative binary matrix factorization
models (NBMF). We analyze three models which correspond to three possible constraints that respect
the mean-parametrization without the need for link functions. Furthermore, we derive a novel collapsed
Gibbs sampler and a collapsed variational algorithm to infer the posterior distribution of the
factors. Next, we extend the proposed models to a nonparametric setting where the number of used
latent dimensions is automatically driven by the observed data. We analyze the performance of our
NBMF methods in multiple datasets for different tasks such as dictionary learning and prediction
of missing data. Experiments show that our methods provide similar or superior results than the
state of the art, while automatically detecting the number of relevant components. 