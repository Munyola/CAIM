Refactoring is a maintenance activity that aims to improve design quality while preserving the
behavior of a system. Several (semi)automated approaches have been proposed to support developers
in this maintenance activity, based on the correction of anti-patterns, which are `poor' solutions
to recurring design problems. However, little quantitative evidence exists about the impact of
automatically refactored code on program comprehension, and in which context automated refactoring
can be as effective as manual refactoring. Leveraging RePOR, an automated refactoring approach
based on partial order reduction techniques, we performed an empirical study to investigate whether
automated refactoring code structure affects the understandability of systems during comprehension
tasks. (1) We surveyed 80 developers, asking them to identify from a set of 20 refactoring changes
if they were generated by developers or by a tool, and to rate the refactoring changes according to
their design quality; (2) we asked 30 developers to complete code comprehension tasks on 10 systems
that were refactored by either a freelancer or an automated refactoring tool. To make comparison
fair, for a subset of refactoring actions that introduce new code entities, only synthetic identifiers
were presented to practitioners. We measured developers' performance using the NASA task load
index for their effort, the time that they spent performing the tasks, and their percentages of correct
answers. Our findings, despite current technology limitations, show that it is reasonable to expect
a refactoring tools to match developer code. 