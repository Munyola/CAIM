We study a Visual-Inertial Navigation (VIN) problem in which a robot needs to estimate its state
using an onboard camera and an inertial sensor, without any prior knowledge of the external environment.
We consider the case in which the robot can allocate limited resources to VIN, due to tight computational
constraints. Therefore, we answer the following question: under limited resources, what are the
most relevant visual cues to maximize the performance of visual-inertial navigation? Our approach
has four key ingredients. First, it is task-driven, in that the selection of the visual cues is guided
by a metric quantifying the VIN performance. Second, it exploits the notion of anticipation, since
it uses a simplified model for forward-simulation of robot dynamics, predicting the utility of
a set of visual cues over a future time horizon. Third, it is efficient and easy to implement, since
it leads to a greedy algorithm for the selection of the most relevant visual cues. Fourth, it provides
formal performance guarantees: we leverage submodularity to prove that the greedy selection cannot
be far from the optimal (combinatorial) selection. Simulations and real experiments on agile drones
show that our approach leads to dramatic improvements in the VIN performance. In the easy scenarios,
our approach outperforms the state-of-the-art in terms of localization errors. In the most challenging
scenarios, it enables accurate visual-inertial navigation while the state of the art fails to track
robot's motion during aggressive maneuvers. 