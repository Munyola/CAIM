Bringing tactile sensation to robotic hands will allow for more effective grasping, along with
the wide range of benefits of human-like touch. Here we present a 3d-printed, three-fingered tactile
robot hand comprising an OpenHand Model~O customized to house a TacTip optical biomimetic tactile
sensor in the distal phalanx of each finger. We expect that the grasping capabilities of the Model
O combined with the benefits of sophisticated tactile sensing will result in an effective platform
-- the tactile Model O (T-MO). Our current T-MO design uses three JeVois machine vision systems,
each comprising a miniature camera in the tactile fingertip with a vision processing module in the
base of the hand. To evaluate the capabilities of the T-MO, we benchmark its grasping performance
using the Gripper Assessment Benchmark on the YCB object set. We then tested its tactile sensing
capabilities with two experiments: firstly, tactile object classification on a subset of objects
that can be reliably grasped, and secondly, predicting whether a grasp will successfully lift one
of these objects under randomly perturbed grasps that sometimes fail. In all cases, the results
are consistent with the state-of-the-art, taking advantage of advances in deep learning and convolutional
neural networks from computer vision that apply to the tactile image outputs. Overall, this work
demonstrates that the T-MO is an effective platform for robot hand research and we expect it to open-up
a range of applications in autonomous object handling. Video: https://youtu.be/oZ41U5pyK6Y
