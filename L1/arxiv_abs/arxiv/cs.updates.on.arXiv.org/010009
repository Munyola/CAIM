Deep neural networks are usually trained with stochastic gradient descent (SGD), which optimizes
$\theta\in\mathbb{R}^D$ parameters to minimize objective function using very rough approximations
of gradient, averaging to the real gradient. To improve its convergence, there is used some state
representing the current situation, like momentum being local average of gradients - gradient
calculated in successive step updates the state, which directly governs the update of $\theta$
parameters. This high dimensional optimization might have more than one direction worth tracing,
also the used methods do not try to estimate distance to the minimum for adapting the step size. Modelling
Hessian would try to trace all the directions, also estimating distance to minimum, however, it
is seen as too costly. We can reduce this representation to make it more practical: try to model some
$d\ll D$ of its most promising (eigen)directions, using $d$ which provides the best tradeoff. There
will be discussed using state as parametrisation with approximated Hessian: $H\approx \sum_{i=1}^d
\lambda_i v_i v_i^T$, preferably using only positive eigenvalues, trying to model the lowest valley
nearby. In every step we can use the calculated gradient to update such local model, then shift the
$\theta$ parameters toward closest minimum of such approximation of local behavior of objective
function. Comparing to the use of single momentum vector, it would remember and update multiple
$(d)$ promising directions, and adapt step size using estimated distance to the minimum. 