In this paper we propose an effective non-rigid object tracking method based on spatial-temporal
consistent saliency detection. In contrast to most existing trackers that use a bounding box to
specify the tracked target, the proposed method can extract the accurate regions of the target as
tracking output, which achieves better description of the non-rigid objects while reduces background
pollution to the target model. Furthermore, our model has several unique features. First, a tailored
deep fully convolutional neural network (TFCN) is developed to model the local saliency prior for
a given image region, which not only provides the pixel-wise outputs but also integrates the semantic
information. Second, a multi-scale multi-region mechanism is proposed to generate local region
saliency maps that effectively consider visual perceptions with different spatial layouts and
scale variations. Subsequently, these saliency maps are fused via a weighted entropy method, resulting
in a final discriminative saliency map. Finally, we present a non-rigid object tracking algorithm
based on the proposed saliency detection method by utilizing a spatial-temporal consistent saliency
map (STCSM) model to conduct target-background classification and using a simple fine-tuning
scheme for online updating. Numerous experimental results demonstrate that the proposed algorithm
achieves competitive performance in comparison with state-of-the-art methods for both saliency
detection and visual tracking, especially outperforming other related trackers on the non-rigid
object tracking datasets. 