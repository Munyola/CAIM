Deep convolutional neural networks (DCNNs) have attracted much attention in remote sensing recently.
Compared with the large-scale annotated dataset in natural images, the lack of labeled data in remote
sensing becomes an obstacle to train a deep network very well, especially in SAR image interpretation.
Transfer learning provides an effective way to solve this problem by borrowing the knowledge from
the source task to the target task. In optical remote sensing application, a prevalent mechanism
is to fine-tune on an existing model pre-trained with a large-scale natural image dataset, such
as ImageNet. However, this scheme does not achieve satisfactory performance for SAR application
because of the prominent discrepancy between SAR and optical images. In this paper, we attempt to
discuss three issues that are seldom studied before in detail: (1) what network and source tasks
are better to transfer to SAR targets, (2) in which layer are transferred features more generic to
SAR targets and (3) how to transfer effectively to SAR targets recognition. Based on the analysis,
a transitive transfer method via multi-source data with domain adaptation is proposed in this paper
to decrease the discrepancy between the source data and SAR targets. Several experiments are conducted
on OpenSARShip. The results indicate that the universal conclusions about transfer learning in
natural images cannot be completely applied to SAR targets, and the analysis of what and where to
transfer in SAR target recognition is helpful to decide how to transfer more effectively. 