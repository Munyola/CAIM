Preserving the utility of published datasets, while providing provable privacy guarantees, is
a well-known challenge. On the one hand, context-free privacy solutions, such as differential
privacy, provide strong privacy guarantees, but often lead to a significant reduction in utility.
On the other hand, context-aware privacy solutions, such as information theoretic privacy, achieve
an improved privacy-utility tradeoff, but assume that the data holder has access to the dataset's
statistics. To circumvent this problem, we present a novel context-aware privacy framework called
generative adversarial privacy (GAP). GAP leverages recent advancements in generative adversarial
networks (GANs) to allow the data holder to learn "optimal" privatization schemes from the dataset
itself. Under GAP, learning the privacy mechanism is formulated as a constrained minimax game between
two players: a privatizer that sanitizes the dataset in a way that limits the risk of inference attacks
on the individuals' private variables, and an adversary that tries to infer the private variables
from the sanitized dataset. To evaluate GAP's performance, we investigate two simple (yet canonical)
statistical dataset models: (a) the binary data model, and (b) the binary Gaussian mixture model.
For both models, we derive game-theoretically optimal minimax privacy mechanisms, and show that
the privacy mechanisms learned from data (in an iterative generative adversarial fashion) match
the theoretically optimal ones. This demonstrates that our framework can be easily applied in practice,
even in the absence of dataset statistics. 